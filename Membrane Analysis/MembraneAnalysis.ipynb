{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook can be used analyze membrane data provided that the data is correctly prepared.\n",
    "\n",
    "The following variables are calculated:\n",
    "- bachwash restoration for each wash cycle\n",
    "- wash cycle length\n",
    "- filtration cycle length\n",
    "- rate of permeability change within each filtration cycle\n",
    "- \"accumulated UV\" in each filtration cycle (UVin * Flux)\n",
    "- temperature compensated TMP after AIT\n",
    "\n",
    "\n",
    "# Contents\n",
    "- User provided settings\n",
    "- Data import and method declaration\n",
    "- Data exploration and computation of variables\n",
    "- Plots, interactive and static\n",
    "- PCA analysis\n",
    "\n",
    "# How to use\n",
    "\n",
    "## Step 1 - Prepare data\n",
    "Import your data by using the script called \"vivab_data_import.ipynb\". Create an ObsVarDef file as in the example file\n",
    "## Step 2 - Run scripts\n",
    "Run each cell in the script using the \"Cell/Run Cells\" button on the top. Some cells requires user input, which is described by each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings\n",
    "User provided input options:\n",
    "\n",
    "- Start date ('yyyy-mm-dd HH:MM:SS')\n",
    "- Stop date ('yyyy-mm-dd HH:MM:SS')\n",
    "- Import excel (True/False). Whether to import excel files or not. If True, the data is stored as a pkl-file which is much faster to load next time.\n",
    "- Median sample rate, in minutes. If other than '1', data is median sampled.\n",
    "- Number of membranes. How many membrane filtration lines are used.\n",
    "- File locations for data file (data_file) and observation and variable description file (obsvardef_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2018-01-01 00:00:00'\n",
    "stop_date = '2020-04-01 00:00:00'\n",
    "median_sample_rate = '1'\n",
    "number_of_membranes = 4\n",
    "import_excel = False\n",
    "data_file = 'joined_data_180101-200401_1m.pkl'\n",
    "obsvardef_file = 'C:/Users/joel0921/OneDrive - IVL Svenska Milj√∂institutet AB/DigiDrick II/VIVAB/ObsVarDef.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import and method declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Python package import\"\"\"\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import scipy.signal as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as sklm\n",
    "import pickle\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Method declarations\"\"\"\n",
    "\n",
    "def find_bw_perm(input_data, perm_name = '_PERM', perm_limit=200, min_distance=24, min_length=10, mean_points=5, plot_results=False):\n",
    "    \n",
    "    \"\"\"Uses permeability drops to find wash cycles\"\"\"\n",
    "    \n",
    "    perm_cols = [s for s in input_data.columns if perm_name in s]  # Find permeability columns\n",
    "    wash_cycle_info = {}  # initialize output dict\n",
    "    \n",
    "    for perm_col in perm_cols:\n",
    "        \n",
    "        wash_cycle_info[perm_col] = pd.DataFrame()\n",
    "        \n",
    "        perm_mask = np.array((input_data[perm_col] < perm_limit) )\n",
    "        perm_mask[np.isnan(perm_mask)] = 1\n",
    "        perm_mask[np.isnan(input_data[perm_col])] = 1\n",
    "        \n",
    "        for ix in range(min_length, len(perm_mask)):\n",
    "            if (perm_mask[ix]==1) & (perm_mask[ix-min_length]==1):\n",
    "                perm_mask[ix-min_length:ix] = 1\n",
    "        \n",
    "        cumsum = np.cumsum(perm_mask)\n",
    "        cumsum_diff = np.diff(cumsum)\n",
    "        \n",
    "        start_ix = []\n",
    "        start_time = []\n",
    "        stop_ix = []\n",
    "        stop_time = []\n",
    "        \n",
    "        for ix in range(1, len(cumsum_diff)):\n",
    "            if (cumsum_diff[ix-1]==0) & (cumsum_diff[ix]==1):  \n",
    "                start_ix.append(ix)\n",
    "                start_time.append(input_data.index[ix])\n",
    "            elif (cumsum_diff[ix-1]==1) & (cumsum_diff[ix]==0):\n",
    "                stop_ix.append(ix+1)\n",
    "                stop_time.append(input_data.index[ix+1])\n",
    "            \n",
    "                \n",
    "        if (len(start_ix) > 1) & (len(stop_ix) > 1):\n",
    "            \n",
    "            first_vals_mean = []\n",
    "            first_vals_mean.append(np.nan)\n",
    "            last_vals_mean = []\n",
    "            last_vals_mean.append(np.nan)\n",
    "            \n",
    "            # Remove last starting point if necessary\n",
    "            if len(start_time) > len(stop_time):\n",
    "                start_time = start_time[:-1]\n",
    "            \"\"\"\n",
    "            # Remove cycles not longer than min_length\n",
    "            remove_count = 0\n",
    "            for ix in range(len(start_time)):\n",
    "                wash_cycle_length = stop_ix[ix] - start_ix[ix]\n",
    "                if wash_cycle_length < min_length:\n",
    "                    del start_time[ix - remove_count]\n",
    "                    del stop_time[ix - remove_count]\n",
    "                    del first_vals_mean[ix - remove_count]\n",
    "                    del last_vals_mean[ix - remove_count]\n",
    "                    remove_count += 1\n",
    "                    \n",
    "            # Remove points where distance to previous CEB is less than min_distance\n",
    "            #last_ceb = stop_time[0]\n",
    "            #for ix in range(1, len(start_time)):\n",
    "            #    if start_time[ix]-last_ceb < datetime.timedelta(hours=min_distance):\n",
    "            #        first_vals_mean[ix] = np.nan\n",
    "            #        last_vals_mean[ix] = np.nan\n",
    "            #    else:\n",
    "            #        last_ceb = stop_time[ix]        \n",
    "                    \n",
    "            \"\"\"        \n",
    "            \n",
    "            if plot_results:\n",
    "                plt.figure(figsize=(15,7))\n",
    "                plt.plot(input_data[perm_col])\n",
    "                plt.plot(input_data.index, perm_mask*100)\n",
    "                plt.title(perm_col)\n",
    "            \n",
    "            # Calculate derivates\n",
    "            reg_coeffs = []\n",
    "            reg_coeffs.append(np.nan)\n",
    "            for ix in range(1, len(start_time)):\n",
    "                intermediate_data = input_data[perm_col].loc[stop_time[ix-1]: start_time[ix]]\n",
    "                intermediate_data = intermediate_data.loc[~np.isnan(intermediate_data)]\n",
    "                \n",
    "                y = intermediate_data.values  \n",
    "                x = np.array(range(len(intermediate_data)))\n",
    "                \n",
    "                first_vals_mean.append(np.median(y[-1*mean_points:]))\n",
    "                last_vals_mean.append(np.median(y[:mean_points]))\n",
    "                \n",
    "                if len(x) > min_length:\n",
    "                    \n",
    "                    remove_values_ix = (y<(np.nanmedian(y)-50)) | (y>(np.nanmedian(y)+50))\n",
    "                    x = x[~remove_values_ix].reshape(-1,1)\n",
    "                    y = y[~remove_values_ix].reshape(-1,1)\n",
    "                    try:\n",
    "                        reg = sklm.LinearRegression().fit(x, y)\n",
    "                    except ValueError:\n",
    "                        break         \n",
    "                            \n",
    "                    if (reg.score(x,y) > 0.5) & (reg.coef_[0][0] < 0):\n",
    "                        reg_coeffs.append(reg.coef_[0][0])\n",
    "                        if plot_results:\n",
    "                            plt.plot(intermediate_data[~remove_values_ix].index, reg.predict(x), 'r*', markersize=3)\n",
    "                    else:\n",
    "                        reg_coeffs.append(np.nan)\n",
    "                        if plot_results:\n",
    "                            plt.plot(intermediate_data[~remove_values_ix].index, reg.predict(x), 'k*', markersize=3)\n",
    "                    \n",
    "                else:\n",
    "                    reg_coeffs.append(np.nan)\n",
    "            \n",
    "            # Remove points where starting value is higher than stopping value\n",
    "            for ix in range(len(first_vals_mean)):\n",
    "                if first_vals_mean[ix] > last_vals_mean[ix]:\n",
    "                    first_vals_mean[ix] = np.nan\n",
    "                    last_vals_mean[ix] = np.nan\n",
    "            \n",
    "            try:\n",
    "                wash_cycle_info[perm_col]['stop_time'] = stop_time\n",
    "                wash_cycle_info[perm_col]['starting_vals'] = first_vals_mean\n",
    "                wash_cycle_info[perm_col]['stopping_vals']= last_vals_mean\n",
    "                wash_cycle_info[perm_col]['start_time'] = start_time\n",
    "                wash_cycle_info[perm_col]['reg_coeffs'] = reg_coeffs \n",
    "            except:\n",
    "                print('Index issues')\n",
    "\n",
    "            if plot_results:\n",
    "                plt.show()\n",
    "    \n",
    "    \n",
    "    output_data = input_data\n",
    "        \n",
    "    return output_data, wash_cycle_info\n",
    "\n",
    "    \n",
    "def calc_wash_cycle_acc254(input_data, ceb_info):\n",
    "    \n",
    "    flux_cols = [s for s in input_data.columns if '_FLUX' in s]  # Find permeability columns\n",
    "    flux_cols = [s for s in flux_cols if '_B' not in s]\n",
    "    flux_cols = [s for s in flux_cols if '_LAST' not in s]\n",
    "    \n",
    "    wash_accUV254 = {}  # initialize output dict\n",
    "    \n",
    "    for flux_col in flux_cols:\n",
    "        \n",
    "        current_filter = flux_col[2:3]\n",
    "        temp_df = pd.DataFrame()\n",
    "        \n",
    "        acc_UV254 = [] # Calculate accumulated UV\n",
    "        acc_vol = [] # Calculate accumulated volume\n",
    "        acc_DeltaUV254 = [] # Calculate accumulated delta UV (UV feed - UV permeate)\n",
    "        cycle_length = []\n",
    "        cycle_datetime = []\n",
    "        mean_flux = []\n",
    "        acc_UV254.append(np.nan)\n",
    "        acc_vol.append(np.nan)\n",
    "        acc_DeltaUV254.append(np.nan)\n",
    "        cycle_length.append(np.nan)\n",
    "        cycle_datetime.append(np.nan)\n",
    "        mean_flux.append(np.nan)\n",
    "        current_ceb = ceb_info['UF' + str(current_filter) + '_PERM']\n",
    "        for ix in range(1, len(current_ceb['start_time'])):\n",
    "            if ~np.isnan(current_ceb['reg_coeffs'][ix]):\n",
    "                intermediate_flux_data = input_data[flux_col].loc[current_ceb['stop_time'][ix-1]: current_ceb['start_time'][ix]]\n",
    "                intermediate_uv_data = input_data['MAT_UV254'].loc[current_ceb['stop_time'][ix-1]: current_ceb['start_time'][ix]]\n",
    "                intermediate_perm_uv_data = input_data['PERM_UV254'].loc[current_ceb['stop_time'][ix-1]: current_ceb['start_time'][ix]]\n",
    "                acc_UV254.append(np.nansum(intermediate_flux_data * intermediate_uv_data))\n",
    "                acc_vol.append(np.nansum(intermediate_flux_data))\n",
    "                acc_DeltaUV254.append(np.nansum(intermediate_flux_data * intermediate_uv_data) - np.nansum(intermediate_flux_data * intermediate_perm_uv_data))\n",
    "                cycle_length.append(len(intermediate_flux_data))\n",
    "                cycle_datetime.append(current_ceb['stop_time'][ix-1])\n",
    "                mean_flux.append(np.nanmean(intermediate_flux_data))\n",
    "        \n",
    "        \n",
    "        \n",
    "        temp_df['AccUV'] = acc_UV254\n",
    "        temp_df['AccVol'] = acc_vol\n",
    "        temp_df['AccDeltaUV'] = acc_DeltaUV254\n",
    "        temp_df['CycleLength'] = cycle_length\n",
    "        temp_df['CycleDateTime'] = cycle_datetime\n",
    "        temp_df['Mean_flux'] = mean_flux\n",
    "        wash_accUV254[flux_col]=temp_df\n",
    "        \n",
    "    return wash_accUV254\n",
    "    \n",
    "    \n",
    "    \n",
    "def exclude_limits(input_data, obsvardef):\n",
    "    output_data = input_data\n",
    "    for col in output_data.columns:\n",
    "        min_limit = obsvardef['Min limit'].loc[obsvardef['Beskrivning']==col].values[0]\n",
    "        max_limit = obsvardef['Max limit'].loc[obsvardef['Beskrivning']==col].values[0]\n",
    "        if ~np.isnan(min_limit):\n",
    "            output_data[col].loc[output_data[col] < min_limit] = np.nan\n",
    "        if max_limit is not None:\n",
    "            output_data[col].loc[output_data[col] > max_limit] = np.nan\n",
    "        \n",
    "    return output_data\n",
    "\n",
    "\n",
    "def remove_bw_data(input_data, wash_info):\n",
    "    print('Removing based on wash_info')\n",
    "    output_data = input_data.copy(deep=True)\n",
    "    for filter_nr in range(1,4):\n",
    "        print('Adding filter to UF' + str(filter_nr))\n",
    "        perm_col = 'UF' + str(filter_nr) + '_PERM'\n",
    "        prod_mask = output_data[perm_col] < np.inf\n",
    "        stop_time = wash_info[perm_col]['stop_time']\n",
    "        start_time = wash_info[perm_col]['start_time']\n",
    "        reg_coeffs = wash_info[perm_col]['reg_coeffs']\n",
    "        for prod_period_ix in range(len(stop_time)-1):\n",
    "            if reg_coeffs[prod_period_ix]!=np.nan:\n",
    "                prod_mask.loc[stop_time[prod_period_ix]:start_time[prod_period_ix + 1]] = False\n",
    "            \n",
    "        output_data['UF' + str(filter_nr) + '_R'].loc[prod_mask] = np.nan\n",
    "        output_data['UF' + str(filter_nr) + '_FLUX'].loc[prod_mask] = np.nan\n",
    "        output_data['UF' + str(filter_nr) + '_PERM'].loc[prod_mask] = np.nan\n",
    "\n",
    "    return output_data\n",
    "\n",
    "\"\"\" Create a Principal Component Analysis model\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.cross_decomposition as skcd\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.model_selection as ms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import f\n",
    "from matplotlib.patches import Ellipse\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "class PCA():\n",
    "\n",
    "    def __init__(self, X, nbr_of_components=2, svd_solver='auto', nan_method='drop'):\n",
    "        \"\"\"Instantiates a PCA model object.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            X : 2D array_like data. Preferably a pandas Dataframe, also handles numpy arrays\n",
    "            nbr_of_components : positive integer, (default = 2)\n",
    "                use number of components in the PCA model.\n",
    "            svd_solver : {‚Äòauto‚Äô, ‚Äòfull‚Äô, ‚Äòarpack‚Äô, ‚Äòrandomized‚Äô}, optional (default = 'auto')\n",
    "                read more about the alternatives at\n",
    "                http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html.\n",
    "            nan_method : {‚Äòlinear‚Äô, ‚Äòtime‚Äô, ‚Äòindex‚Äô, ‚Äòvalues‚Äô, ‚Äònearest‚Äô, ‚Äòzero‚Äô,‚Äòslinear‚Äô, ‚Äòquadratic‚Äô, ‚Äòcubic‚Äô,\n",
    "                        ‚Äòbarycentric‚Äô, ‚Äòkrogh‚Äô, ‚Äòpolynomial‚Äô, ‚Äòspline‚Äô, ‚Äòpiecewise_polynomial‚Äô, ‚Äòfrom_derivatives‚Äô,\n",
    "                        ‚Äòpchip‚Äô, ‚Äòakima‚Äô}, (default = 'drop')\n",
    "                Use the method to either drop out rows containing NaNs or interpolate using one of the said methods.\n",
    "\n",
    "\n",
    "            Notes\n",
    "            -----\n",
    "            Suggestions for improvement\n",
    "            - Interactive plots in matplotlib? Examining the contribution in outliers as in SIMCA and color mark extremes\n",
    "            - Or define as method for all points with dmodx > certain limit\n",
    "            - Cross variance matrix for both PCA and PLS \"\"\"\n",
    "\n",
    "        # Convert X to pandas.DataFrame if not already\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            try:\n",
    "                X = pd.DataFrame(X)\n",
    "            except TypeError:\n",
    "                raise TypeError('Format of input X is not supported. Accepted formats: pandas.DataFrame or numpy.ndarray')\n",
    "\n",
    "        # Handle NaNs using the chosen method [default = drop]\n",
    "        if nan_method == 'drop':\n",
    "            X = X.dropna()\n",
    "        else:\n",
    "            try:\n",
    "                X = X.interpolate(nan_method)\n",
    "            except ValueError:\n",
    "                raise ValueError('Method name \"' + nan_method +\n",
    "                                 '\" not supported. See documentation for pandas.DataFrame.interpolate for acceptable methods.')\n",
    "\n",
    "        scalerX = StandardScaler().fit(X) # Scale X, remove mean and scale to unit variance\n",
    "        self.scaledX = pd.DataFrame(scalerX.transform(X), index=X.index.values, columns=X.columns.values)\n",
    "\n",
    "        pca = skd.PCA(n_components=nbr_of_components, svd_solver=svd_solver)\n",
    "        self.scores = pca.fit_transform(self.scaledX)\n",
    "        self.loadings = pca.components_\n",
    "\n",
    "        x_hat = np.dot(self.scores, self.loadings)\n",
    "        model_residual = np.array(self.scaledX) - x_hat\n",
    "        self.dModX = np.sqrt((np.sum(np.square(model_residual), axis=1)) / np.var(model_residual))\n",
    "\n",
    "        self.explained_variance = pca.explained_variance_ratio_\n",
    "        self.eigenvalues = pca.explained_variance_\n",
    "        self.R2 = sum(self.explained_variance)\n",
    "        self.correlation_matrix = self.scaledX.corr()\n",
    "\n",
    "    def plot_scores(self, components=(0, 1), alpha=0.05, c_index=0, s=1):\n",
    "        confidence_level = 100 * (1 - alpha) # Confidence level in percent\n",
    "\n",
    "        ax = plt.subplot(111)\n",
    "        N = len(self.scaledX)  # Number of observations\n",
    "\n",
    "        # Create Hotelling's T2 ellipse axis\n",
    "        ht2_axis_a = np.sqrt(np.var(self.scores[:, components[0]]) * f.ppf(1 - alpha, 2, N - 2) * 2 * (N - 1) / (N - 2))\n",
    "        ht2_axis_b = np.sqrt(np.var(self.scores[:, components[1]]) * f.ppf(1 - alpha, 2, N - 2) * 2 * (N - 1) / (N - 2))\n",
    "\n",
    "        ellipse = Ellipse((0, 0), 2 * ht2_axis_a, 2 * ht2_axis_b)  # Create Hotelling's T2 ellipse\n",
    "        ellipse.set_alpha(0.1)  # Set transparency\n",
    "\n",
    "        ax.add_artist(ellipse)\n",
    "        plt.xlabel('t[' + str(components[0]) + ']')\n",
    "        plt.ylabel('t[' + str(components[1]) + ']')\n",
    "        plt.grid()\n",
    "        try:\n",
    "            color_index = [i[c_index] for i in self.scaledX.index]\n",
    "        except:\n",
    "            print('Setting index as coloring..')\n",
    "            color_index = self.scaledX.index.values\n",
    "        plt.scatter(self.scores[:, components[0]], self.scores[:, components[1]], s=s, c=color_index, cmap='jet') # Plot scores\n",
    "        fig_string = '\\n R2X[' + str(components[0]) + '] = ' + '%.2f' % self.explained_variance[components[0]] + '\\t R2X['\\\n",
    "                     + str(components[1]) + '] = ' + '%.2f' % self.explained_variance[components[1]] \\\n",
    "                     + '\\t Ellipse: Hotelling''s T2 (' + '%.0f' % confidence_level + '%)'\n",
    "        ax.annotate(fig_string.expandtabs(), xy=(1, 1), xytext=(0.5, 0.01), textcoords='figure fraction',\n",
    "                    horizontalalignment='center')\n",
    "        plt.title('Score plot')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_loadings(self, components=(0, 1)):\n",
    "\n",
    "        ax = plt.subplot(111)\n",
    "        plt.xlabel('p[' + str(components[0]) + ']')\n",
    "        plt.ylabel('p[' + str(components[1]) + ']')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.scatter(self.loadings.T[:, components[0]], self.loadings.T[:, components[1]], s=2)\n",
    "        fig_string = 'R2X[' + str(components[0]) + '] = ' + '%.2f' % self.explained_variance[components[0]] + '\\t R2X['\\\n",
    "                     + str(components[1]) + '] = ' + '%.2f' % self.explained_variance[components[1]]\n",
    "        ax.annotate(fig_string.expandtabs(), xy=(1, 1),  xycoords='figure points', xytext=(0.5, 0.01),\n",
    "                    textcoords='figure fraction', horizontalalignment='center')\n",
    "        for idx, element in enumerate(self.loadings.T):\n",
    "            ax.annotate(self.scaledX.columns.values[idx], xy=(element[0], element[1]), xycoords='data')\n",
    "\n",
    "        # Plot lines to mark axis\n",
    "        axes = plt.gca()\n",
    "        xlim = (-1, 1)  #axes.get_xlim()\n",
    "        ylim = (-1, 1)  #axes.get_ylim()\n",
    "        plt.hlines(0, min(xlim), max(xlim))\n",
    "        plt.vlines(0, min(ylim), max(ylim))\n",
    "        axes.set_xlim(xlim)\n",
    "        axes.set_ylim(ylim)\n",
    "        plt.title('Loading plot')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_scores_3D(self, components = (0, 1, 2), alpha = 0.05):\n",
    "\n",
    "        ax = plt.subplot(111, aspect='equal', projection='3d')\n",
    "        N = len(self.scaledX)  # Number of observations\n",
    "\n",
    "        # Create Hotelling's T2 ellipsoid axes\n",
    "        ht2_axis_a = np.sqrt(np.var(self.scores[:, components[0]]) * f.ppf(1 - alpha, 2, N - 2) * 2 * (N - 1) / (N - 2))\n",
    "        ht2_axis_b = np.sqrt(np.var(self.scores[:, components[1]]) * f.ppf(1 - alpha, 2, N - 2) * 2 * (N - 1) / (N - 2))\n",
    "        ht2_axis_c = np.sqrt(np.var(self.scores[:, components[2]]) * f.ppf(1 - alpha, 2, N - 2) * 2 * (N - 1) / (N - 2))\n",
    "\n",
    "        # Set of all spherical angles:\n",
    "        u = np.linspace(0, 2 * np.pi, 100)\n",
    "        v = np.linspace(0, np.pi, 100)\n",
    "        # Cartesian coordinates that correspond to the spherical angles:\n",
    "        # (this is the equation of an ellipsoid):\n",
    "        x = ht2_axis_a * np.outer(np.cos(u), np.sin(v))\n",
    "        y = ht2_axis_b * np.outer(np.sin(u), np.sin(v))\n",
    "        z = ht2_axis_c * np.outer(np.ones_like(u), np.cos(v))\n",
    "\n",
    "        # Plot Hotelling's T2 ellipsoid:\n",
    "        ax.plot_surface(x, y, z, rstride=4, cstride=4, alpha=0.1)\n",
    "\n",
    "        plt.xlabel('t[' + str(components[0]) + ']')\n",
    "        plt.ylabel('t[' + str(components[1]) + ']')\n",
    "        plt.grid()\n",
    "        x = self.scores[:, components[0]]\n",
    "        y = self.scores[:, components[1]]\n",
    "        z = self.scores[:, components[2]]\n",
    "        ax.set_zlabel('t[' + str(components[2]) + ']')\n",
    "        ax.scatter(x, y, z, s=1, c=self.scaledX.index.values)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data import and change of column names\n",
    "The data is imported and variables are given the names provided in the \"Beskrivning\" column of ObsVarDef.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data import\"\"\"\n",
    "\n",
    "obsvardef_data = pd.read_excel(obsvardef_file)\n",
    "input_data = pd.read_pickle(data_file)\n",
    "\n",
    "old_names = list(obsvardef_data['VarNam'])\n",
    "new_names = list(obsvardef_data['Beskrivning'])\n",
    "\n",
    "zipbObj = zip(old_names, new_names)\n",
    "dict_keys = dict(zipbObj)\n",
    "\n",
    "\n",
    "col_mod = []\n",
    "for col in input_data.columns:\n",
    "    for column, new_name in dict_keys.items():\n",
    "        col = col.replace(column, new_name)\n",
    "    col_mod.append(col)\n",
    "\n",
    "input_data.columns = col_mod\n",
    "input_data = input_data.reindex(sorted(input_data.columns), axis=1)\n",
    "    \n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre-process data\"\"\"\n",
    "data_of_interest = exclude_limits(input_data, obsvardef_data)\n",
    "data_limits_applied = data_of_interest.resample(median_sample_rate + 'T').median()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find filtration and wash cycle data\n",
    "- perm_name: the name of the permeability signal \n",
    "- perm_limit: permeability limit to find where the washing cycle begins\n",
    "- min_distance: minimum number of sampling points between wash cycles\n",
    "- min_length: minimum wash cycle length in sampling points\n",
    "- mean_points: number of points over which the start and stop values are averaged\n",
    "- plot_results: whether to plot the results or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find filtration and wash cycle data using the permeability\n",
    "\"\"\"\n",
    "x, wash_info = find_bw_perm(data_limits_applied, perm_name = '_PERM', perm_limit=200, min_distance=0, min_length=20, mean_points=10, plot_results=False)\n",
    "wash_uv = calc_wash_cycle_acc254(x, wash_info)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create dataset with calculated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_variables = pd.DataFrame()\n",
    "\n",
    "for ix in range(1, number_of_membranes + 1):\n",
    "    wash_time_d = wash_info['UF' + str(ix) + '_PERM']['stop_time']-wash_info['UF' + str(ix) + '_PERM']['start_time']\n",
    "    wash_cycle_length = wash_time_d.dt.total_seconds()/60\n",
    "    prod_time_d = wash_info['UF' + str(ix) + '_PERM']['start_time']-wash_info['UF' + str(ix) + '_PERM']['stop_time'].shift(1)\n",
    "    prod_cycle_length = prod_time_d.dt.total_seconds()/60\n",
    "    restoration = wash_info['UF' + str(ix) + '_PERM']['stopping_vals']-wash_info['UF' + str(ix) + '_PERM']['starting_vals']\n",
    "    calculated_variables['restoration_UF' + str(ix)] = restoration\n",
    "    calculated_variables['wash_cyclelength_UF' + str(ix)] = wash_cycle_length\n",
    "    calculated_variables['prod_cyclelength_UF' + str(ix)] = prod_cycle_length\n",
    "    calculated_variables['derivative_UF' + str(ix)] = wash_info['UF' + str(ix) + '_PERM']['reg_coeffs']\n",
    "    calculated_variables['AccUV_UF' + str(ix)] = wash_uv['UF' + str(ix) + '_FLUX']['AccUV']\n",
    "    calculated_variables['cycle_datetime_UF' + str(ix)] = wash_info['UF' + str(ix) + '_PERM']['start_time']\n",
    "    calculated_variables['accUV_cycle_datetime_UF' + str(ix)] = wash_uv['UF' + str(ix) + '_FLUX']['CycleDateTime']\n",
    "\n",
    "print('Done')\n",
    "#calculated_variables.to_csv('calculated_vars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find LAST TMP after AIT and temperature compensate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = data_limits_applied['TEMP_CALC']\n",
    "last_tmp_array = []\n",
    "for uf_ix in range(1, number_of_membranes + 1):\n",
    "    \n",
    "    last_tmp = data_limits_applied['UF' + str(uf_ix) + '_L_TMP']\n",
    "    ait_beg = data_limits_applied['UF' + str(uf_ix) + '_AIT_BEG']\n",
    "    ait_beg_times = data_limits_applied.index[np.diff(ait_beg, append=1)==-1]\n",
    "    tmp_diff_times = data_limits_applied.index[np.diff(last_tmp, append=1)!=0]\n",
    "    tmp_df = last_tmp[tmp_diff_times]\n",
    "    values = []\n",
    "    for ix in range(len(ait_beg_times)-1):\n",
    "        first_value_after = np.max(tmp_df[ait_beg_times[ix]:ait_beg_times[ix+1]])\n",
    "        correction = 0.002024 * ((42.5 + temperature[ait_beg_times[ix]])**1.5)   \n",
    "        values.append(first_value_after*correction)\n",
    "    \n",
    "    last_tmp_array.append(pd.DataFrame(data=values, index=ait_beg_times[:-1], columns=['UF' + str(uf_ix) + '_LAST_TMP_CORR']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge input data and calculated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = data_limits_applied.copy()\n",
    "for ix in range(1, number_of_membranes + 1):\n",
    "    \n",
    "    temp_df_1 = wash_info['UF' + str(ix) + '_PERM'][['stopping_vals']]\n",
    "    temp_df_1.index = pd.to_datetime(wash_info['UF' + str(ix) + '_PERM']['stop_time'])\n",
    "    temp_df_1.columns = temp_df_1.columns + '_UF' + str(ix)\n",
    "    \n",
    "    temp_df_2 = wash_info['UF' + str(ix) + '_PERM'][['starting_vals']]\n",
    "    temp_df_2.index = pd.to_datetime(wash_info['UF' + str(ix) + '_PERM']['start_time'])\n",
    "    temp_df_2.columns = temp_df_2.columns + '_UF' + str(ix)\n",
    "    \n",
    "    temp_df_3 =  calculated_variables[['restoration_UF' + str(ix), 'wash_cyclelength_UF' + str(ix), 'prod_cyclelength_UF' + str(ix), 'derivative_UF' + str(ix)]]\n",
    "    temp_df_3.index = pd.to_datetime(calculated_variables['cycle_datetime_UF' + str(ix)])\n",
    "    \n",
    "    merged_wash_info = temp_df_1.join([temp_df_2, temp_df_3, last_tmp_array[ix-1]], how='outer')\n",
    "    merged_wash_info_resampled = merged_wash_info.resample('1T').median().ffill().dropna()\n",
    "    all_vars = all_vars.join(merged_wash_info_resampled)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview plotting - interactive\n",
    "- Run the cell\n",
    "- Use the start and stop dates to select date interval\n",
    "- Set the number of minutes to use for median filter (minimum 1)\n",
    "- Choose which variables to plot, hold ctrl-key to select multiple options. \n",
    "- The more variables and high time resolution will take more time to load\n",
    "- The figure is updated when clicking \"Run Interact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, Layout\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "plot_df = all_vars\n",
    "\n",
    "start_date = widgets.DatePicker(description='Start', disabled=False, value=pd.to_datetime('2018-01-01'))\n",
    "display(start_date)\n",
    "stop_date = widgets.DatePicker(description='Stop', disabled=False, value=pd.to_datetime('2020-04-01'))\n",
    "display(stop_date)\n",
    "\n",
    "median_slider = widgets.IntSlider(min=1, max=60, step=1, value=10, description='Median filter [min]')\n",
    "display(median_slider)\n",
    "\n",
    "list_layout = widgets.Layout(width='100%')\n",
    "\n",
    "\n",
    "y_list_1 = widgets.SelectMultiple(\n",
    "    options=list(plot_df.select_dtypes('number').columns),\n",
    "    value=[plot_df.select_dtypes('number').columns[0]],\n",
    "    description='Left Y',\n",
    "    disabled=False, layout=Layout(width='40%', height='200px'))\n",
    "\n",
    "y_list_2 = widgets.SelectMultiple(\n",
    "    options=list(plot_df.select_dtypes('number').columns),\n",
    "    value=[plot_df.select_dtypes('number').columns[0]],\n",
    "    description='Right Y',\n",
    "    disabled=False, layout=Layout(width='40%', height='200px'))\n",
    "\n",
    "lists = widgets.HBox(children=[y_list_1, y_list_2],layout=list_layout)\n",
    "#display(lists)\n",
    "\n",
    "\n",
    "def update_plot(y_1=y_list_1, y_2=y_list_2):\n",
    "    \n",
    "    df = all_vars.resample(str(median_slider.value) + 'T').median()\n",
    "    df = df.loc[start_date.value:stop_date.value]\n",
    "    list_1 = list(y_1)\n",
    "    list_2 = list(y_2)\n",
    "    plot_df = df[list_1 + list_2]\n",
    "    display(plot_df.iplot(secondary_y=list_2))\n",
    "    \n",
    "man = interact_manual(update_plot, y1=y_list_1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview scatter plot for calculated variables\n",
    "- Run the cell\n",
    "- The figure is updated when changing either of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def scatter_plot(x=list(calculated_variables.select_dtypes('number').columns), \n",
    "                y= list(calculated_variables.select_dtypes('number').columns)[1:]):\n",
    "    display(calculated_variables.iplot(x=x, y=y, kind='scatter', xTitle=x.title(), yTitle=y.title(), mode='markers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "The following segment creates static plots described by their header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwash restoration - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "for ix in range(1, number_of_membranes + 1):\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    \n",
    "    plt.title('Backwash restoration, UF' + str(ix))\n",
    "    plt.plot(wash_info['UF' + str(ix) + '_PERM']['stop_time'], \n",
    "             wash_info['UF' + str(ix) + '_PERM']['stopping_vals']-wash_info['UF' + str(ix) + '_PERM']['starting_vals'], 'r*', markersize=2)\n",
    "    plt.ylabel('Backwash restoration [l / h m2 bar]')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtration cycle derivative - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.title('Filtration cycle derivative, UF' + str(ix))\n",
    "    plt.plot(wash_info['UF' + str(ix) + '_PERM']['stop_time'], wash_info['UF' + str(ix) + '_PERM']['reg_coeffs'], 'r*',markersize=2)\n",
    "    plt.ylabel('Filtration cycle derivative [permeability/min]')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMP after AIT temperature compensated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "for ix in range(1, number_of_membranes + 1):\n",
    "    \n",
    "    plt.title('TMP after AIT')\n",
    "    tmp = last_tmp_array[ix-1]['UF' + str(ix) + '_LAST_TMP_CORR']\n",
    "    plt.plot(tmp,  '*',color=c[ix-1], markersize=5, label='UF1:' + str(ix))\n",
    "    plt.ylabel('TMP [bar]')\n",
    "    \n",
    "    y = tmp.dropna().values  \n",
    "    x = np.array(range(len(y)))\n",
    "    \n",
    "    remove_values_ix = (y<(np.nanmedian(y)-0.1)) | (y>(np.nanmedian(y)+0.1))\n",
    "    x = x[~remove_values_ix].reshape(-1,1)\n",
    "    y = y[~remove_values_ix].reshape(-1,1)\n",
    "    reg = sklm.LinearRegression().fit(x, y)\n",
    "    plt.plot(tmp.dropna().index[~remove_values_ix], reg.predict(x), color=c[ix-1], linewidth=0.5, label='k=' + '{:.2E}'.format(reg.coef_[0][0]))\n",
    "    print(reg.coef_)\n",
    "\n",
    "#plt.minorticks_on()\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and stop points for each filtration cycle - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "for ix in range(1, number_of_membranes + 1):\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.title('Permeability start and stopping points in each filtration cycle, UF' + str(ix))\n",
    "    plt.plot(data_limits_applied['UF' + str(ix) + '_PERM'])\n",
    "    plt.plot(wash_info['UF' + str(ix) + '_PERM']['start_time'],wash_info['UF' + str(ix) + '_PERM']['starting_vals'], '*', markersize=5)\n",
    "    plt.plot(wash_info['UF' + str(ix) + '_PERM']['stop_time'],wash_info['UF' + str(ix) + '_PERM']['stopping_vals'], '*', markersize=5)\n",
    "    plt.legend(['Values', 'Filtration cycle end point','Filtration cycle start point'], markerscale=5)\n",
    "    plt.ylabel('Permeability [l / h m2 bar]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.title('Permeability start and stopping points in each filtration cycle, UF' + str(ix))\n",
    "    plt.plot(wash_info['UF' + str(ix) + '_PERM']['start_time'],wash_info['UF' + str(ix) + '_PERM']['starting_vals'], '*', markersize=1)\n",
    "    plt.plot(wash_info['UF' + str(ix) + '_PERM']['stop_time'],wash_info['UF' + str(ix) + '_PERM']['stopping_vals'], '*', markersize=1)\n",
    "    plt.legend(['Filtration cycle end point','Filtration cycle start point'], markerscale=5)\n",
    "    plt.ylabel('Permeability [l / h m2 bar]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwash resotration + temperature - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "    c = 'tab:blue'\n",
    "    plt.title('UF' + str(ix))\n",
    "    ax1.plot(wash_info['UF' + str(ix) + '_PERM']['stop_time'], \n",
    "             wash_info['UF' + str(ix) + '_PERM']['stopping_vals']-wash_info['UF' + str(ix) + '_PERM']['starting_vals'], '*', markersize=1, color=c)\n",
    "    ax1.tick_params(axis='y', labelcolor=c)\n",
    "    ax1.set_ylabel('Backwash restoration [l / h m2 bar]', color=c)\n",
    "    #ax1.set_ylim([150, 450])\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    c = 'tab:orange'\n",
    "    ax2.plot(data_limits_applied['TEMP_CALC'], '*', markersize=1, color=c)\n",
    "    ax2.tick_params(axis='y', labelcolor=c)\n",
    "    ax2.set_ylabel('Temperature [C]', color=c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wash cycle restoration vs filtration cycle derivatives - scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for ix in range(1,number_of_membranes + 1):\n",
    "    plt.figure()\n",
    "    plt.title('UF' + str(ix))\n",
    "    plt.scatter(calculated_variables['restoration_UF' + str(ix)], calculated_variables['derivative_UF' + str(ix)], s=2)\n",
    "    plt.xlabel('Wash cycle restoration [l / h m2 bar]')\n",
    "    plt.ylabel('Filtration cycle derivative [permeability/min]')\n",
    "    plt.xlim([0, 150])\n",
    "    plt.ylim([-1.5, 0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtration cycle derivative + filtration cycle length - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "    c = 'tab:blue'\n",
    "    plt.title('UF' + str(ix))\n",
    "    ax1.plot(calculated_variables['cycle_datetime_UF' + str(ix)],calculated_variables['prod_cyclelength_UF' + str(ix)], '*', markersize=1, color=c)\n",
    "    ax1.tick_params(axis='y', labelcolor=c)\n",
    "    ax1.set_ylabel('Filtration cycle length [min]', color=c)\n",
    "    ax1.set_ylim([0, 400])\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    c = 'tab:orange'\n",
    "    ax2.plot(calculated_variables['cycle_datetime_UF' + str(ix)], calculated_variables['derivative_UF'+ str(ix)], '*', markersize=1, color=c)\n",
    "    ax2.tick_params(axis='y', labelcolor=c)\n",
    "    ax2.set_ylabel('Filtration cycle derivative [permeability/min]', color=c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtration cycle derivative + Frac GV - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "for ix in range(1,number_of_membranes + 1):\n",
    "    fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "    c = 'tab:blue'\n",
    "    plt.title('UF' + str(ix))\n",
    "    ax1.plot(data_limits_applied['FRAC_GV'], '*', markersize=1, color=c)\n",
    "    ax1.tick_params(axis='y', labelcolor=c)\n",
    "    ax1.set_ylabel('Fraction GV [%]', color=c)\n",
    "    ax1.set_ylim([0, 35])\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    c = 'tab:orange'\n",
    "    ax2.plot(calculated_variables['cycle_datetime_UF' + str(ix)], calculated_variables['derivative_UF'+ str(ix)], '*', markersize=1, color=c)\n",
    "    ax2.tick_params(axis='y', labelcolor=c)\n",
    "    ax2.set_ylabel('Filtration cycle derivative [permability/min]', color=c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtration cycle derivative + filtration flux - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "    c = 'tab:blue'\n",
    "    plt.title('UF' + str(ix))\n",
    "    ax1.plot(calculated_variables['cycle_datetime_UF' + str(ix)], calculated_variables['derivative_UF'+ str(ix)], '*', markersize=1, color=c)\n",
    "    ax1.set_ylabel('Filtration cycle derivative', color=c)\n",
    "    ax1.tick_params(axis='y', labelcolor=c)\n",
    "    \n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    c = 'tab:orange'\n",
    "    ax2.plot(wash_uv['UF' + str(ix) + '_FLUX']['CycleDateTime'],wash_uv['UF' + str(ix) + '_FLUX']['Mean_flux'], '*', markersize=1, color=c)\n",
    "    ax2.set_ylabel('Average filtration flux', color=c)\n",
    "    ax2.tick_params(axis='y', labelcolor=c)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ackumulativ UV254 + matarvatten UV254 - tidsserie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "    c = 'tab:blue'\n",
    "    plt.title('Accumulative UV254 vs feed water UV254, UF' + str(ix))\n",
    "    ax1.plot(data_limits_applied['MAT_UV254'], '*', markersize=1, color=c)\n",
    "    #ax1.plot(data_limits_applied['PERM_UV254'], '*', markersize=1, color='k')\n",
    "    ax1.tick_params(axis='y', labelcolor=c)\n",
    "    ax1.set_ylabel('Feed UV254', color=c)\n",
    "    #plt.legend(['', 'PERM_UV254', ''])\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    c = 'tab:orange'\n",
    "    ax2.plot(calculated_variables['accUV_cycle_datetime_UF' + str(ix)], calculated_variables['AccUV_UF' + str(ix)], '*', markersize=1, color=c)\n",
    "    ax2.tick_params(axis='y', labelcolor=c)\n",
    "    ax2.set_ylabel('Accumulative UF254', color=c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulated UV254 + derivatives - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "    c = 'tab:blue'\n",
    "    plt.title('Accumulative UV254 vs filtration cycle derivate, UF' + str(ix))\n",
    "    ax1.plot(calculated_variables['cycle_datetime_UF' + str(ix)], calculated_variables['derivative_UF'+ str(ix)], '*', markersize=1, color=c)\n",
    "    ax1.tick_params(axis='y', labelcolor=c)\n",
    "    ax1.set_ylabel('Filtration cycle derivative', color=c)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    c = 'tab:orange'\n",
    "    ax2.plot(calculated_variables['accUV_cycle_datetime_UF' + str(ix)], calculated_variables['AccUV_UF' + str(ix)], '*', markersize=1, color=c)\n",
    "    ax2.tick_params(axis='y', labelcolor=c)\n",
    "    ax2.set_ylabel('Accumulative UF254', color=c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulated UV254 vs PERM starting point - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "    c = 'tab:blue'\n",
    "    plt.title('Accumulative UV254 vs PERM after BW, UF' + str(ix))\n",
    "    \n",
    "    ax1.plot(wash_info['UF' + str(ix) + '_PERM']['start_time'], wash_info['UF' + str(ix) + '_PERM']['starting_vals'], '*', markersize=1, color=c)\n",
    "    ax1.tick_params(axis='y', labelcolor=c)\n",
    "    ax1.set_ylabel('Permeability after backwash', color=c)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    c = 'tab:orange'\n",
    "    ax2.plot(calculated_variables['accUV_cycle_datetime_UF' + str(ix)], calculated_variables['AccUV_UF' + str(ix)], '*', markersize=1, color=c)\n",
    "    ax2.tick_params(axis='y', labelcolor=c)\n",
    "    ax2.set_ylabel('Accumulative UF254', color=c)\n",
    "    ax2.set_ylim([0, 70000])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulated UV 254 vs Filtration cycle length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    plt.figure()\n",
    "    x = np.array(wash_uv['UF' + str(ix) + '_FLUX']['CycleLength'].dropna()).reshape(-1,1)\n",
    "    y = np.array(wash_uv['UF' + str(ix) + '_FLUX']['AccUV'].dropna()).reshape(-1,1)\n",
    "    plt.scatter(x, y, s=1, c='k')\n",
    "    elastic = sklm.ElasticNet().fit(x, y)\n",
    "    lasso= sklm.Lasso().fit(x, y)\n",
    "    linear = sklm.LinearRegression().fit(x, y)\n",
    "    ridge = sklm.Ridge().fit(x, y)\n",
    "    plt.plot(x, elastic.predict(x), 'r')\n",
    "    plt.plot(x, ridge.predict(x), 'b')\n",
    "    plt.plot(x, linear.predict(x), 'g')\n",
    "    plt.plot(x, lasso.predict(x), 'm')\n",
    "    print(ridge.coef_)\n",
    "    plt.xlim([0, 400])\n",
    "    plt.ylim([0, 40000])\n",
    "    plt.ylabel('Accumulated UV254')\n",
    "    plt.xlabel('Cycle length')\n",
    "    plt.title('UF' + str(ix) + ' acc UV254 / cycle length')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwash restoration vs feed dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1,number_of_membranes + 1):\n",
    "    fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "    c = 'tab:blue'\n",
    "    plt.title('Backwash restoration vs feed dose, UF' + str(ix))\n",
    "    ax1.plot(data_limits_applied['PAC_DOS'].resample('5H').median(), '*', markersize=1, color=c)\n",
    "    ax1.tick_params(axis='y', labelcolor=c)\n",
    "    ax1.set_ylabel('Feed dose', color=c)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    c = 'tab:orange'\n",
    "    ax2.plot(calculated_variables['cycle_datetime_UF' + str(ix)], calculated_variables['restoration_UF' + str(ix)], '*', markersize=1, color=c)\n",
    "    ax2.tick_params(axis='y', labelcolor=c)\n",
    "    ax2.set_ylabel('Backwash restoration', color=c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulated volume  + filtration cycle length - time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ix in range(1, number_of_membranes + 1):\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "    c = 'tab:blue'\n",
    "    plt.title('Cycle limits, UF1:' + str(ix))\n",
    "    ax1.plot(wash_uv['UF' + str(ix) + '_FLUX']['CycleDateTime'], wash_uv['UF' + str(ix) + '_FLUX']['AccVol'], '*', markersize=1, color=c)\n",
    "    ax1.tick_params(axis='y', labelcolor=c)\n",
    "    ax1.set_ylabel('Accumulated volume', color=c)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    c = 'tab:orange'\n",
    "    ax2.plot(wash_uv['UF' + str(ix) + '_FLUX']['CycleDateTime'], wash_uv['UF' + str(ix) + '_FLUX']['CycleLength'], '*', markersize=1, color=c)\n",
    "    ax2.tick_params(axis='y', labelcolor=c)\n",
    "    ax2.set_ylabel('Cycle length', color=c)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed and permeate quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "c = 'tab:blue'\n",
    "plt.title('Feed water quality')\n",
    "ax1.plot(all_vars['MAT_COLOR'], '*', markersize=1, color=c)\n",
    "ax1.tick_params(axis='y', labelcolor=c)\n",
    "ax1.set_ylabel('Color [Pt-Co]', color=c)\n",
    "ax1.set_ylim([5, 30])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "c = 'tab:orange'\n",
    "ax2.plot(all_vars['MAT_UV254'], '*', markersize=1, color=c)\n",
    "ax2.tick_params(axis='y', labelcolor=c)\n",
    "ax2.set_ylabel('UV254 [1/m]', color=c)\n",
    "ax2.set_ylim([4, 12])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(9,4))\n",
    "c = 'tab:blue'\n",
    "plt.title('Permeate water quality')\n",
    "ax1.plot(all_vars[['PERM_COLOR', 'PERM_COLOR_2']], '*', markersize=1, color=c)\n",
    "ax1.tick_params(axis='y', labelcolor=c)\n",
    "ax1.set_ylabel('Color [Pt-Co]', color=c)\n",
    "ax1.set_ylim([1, 6])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "c = 'tab:orange'\n",
    "ax2.plot(all_vars[['PERM_UV254', 'PERM_UV254_2']],'*', markersize=1, color=c)\n",
    "ax2.tick_params(axis='y', labelcolor=c)\n",
    "ax2.set_ylabel('UV254 [1/m]', color=c)\n",
    "ax2.set_ylim([2, 6])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA - principal component analysis\n",
    "Principal component analysis is used to gain further understanding of the relationship between variables.\n",
    "Find further details here: http://miljostatistik.se/PCA.html\n",
    "\n",
    "- Score plot: Each point is a measurement point, colored by time (blue=start, red=stop). <br/> R2X[0] + R2X[1] is an estimation on how much of the variance in the data that is explained by the first two principal components (t0 and t1). This should be as close to 1 as possible. \n",
    "- Loading plot: Each point is a specific variable. The plot shows how the principal components depend on the input variables. <br/> Variables close to eachother are positively correlated <br/> Variables on opposing side of the origin are negatively correlated <br/> Orthogonal variables are not correlated.\n",
    "\n",
    "Analyzing the combination of score and loading plots can make you draw conclusions on how the process is changing over time. If for example the points in the score plot are moving towards the upper left corner, it probably indicates an increase of the variables in the upper left corner in the loading plot (or decrease of variables in the lower right corner that are negatively correlated). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA of calculated variables\n",
    "The calculated variables for each membrane is used for PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# import PCA\n",
    "pca_df = calculated_variables.select_dtypes(exclude=['datetime'])\n",
    "\n",
    "for ix in range(1,number_of_membranes + 1):\n",
    "    cols = [s for s in pca_df.columns if str(ix) in s]\n",
    "    pca = PCA(pca_df[cols])\n",
    "    plt.figure(figsize=(7,7))\n",
    "    pca.plot_scores(s=3)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    pca.plot_loadings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA of original variables\n",
    "Run the script and select which variables you would like to use for PCA analysis. Multiple selections are done by holding the ctrl-key. To run the analysis, click \"Run interaction\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "pca_df = all_vars.select_dtypes(exclude=['datetime'])  \n",
    "y_list = widgets.SelectMultiple(\n",
    "    options=list(pca_df.select_dtypes('number').columns),\n",
    "    value=[pca_df.select_dtypes('number').columns[0]],\n",
    "    description='Input',\n",
    "    disabled=False, rows=len(list(pca_df.select_dtypes('number').columns)))\n",
    "\n",
    "def update_plot(y=y_list):   \n",
    "        \n",
    "    plot_df = pca_df[list(y)]\n",
    "    pca = PCA(plot_df.dropna(axis=1, how='all').dropna(axis=0))\n",
    "    #pca = PCA(pca_df[cols].fillna(method='backfill').fillna(method='ffill').dropna(axis=1))\n",
    "    plt.figure(figsize=(7,7))\n",
    "    pca.plot_scores(s=3)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    pca.plot_loadings()\n",
    "\n",
    "\n",
    "interact_manual(update_plot, y_list = widgets.SelectMultiple(\n",
    "    options=list(pca_df.select_dtypes('number').columns),\n",
    "    value=[pca_df.select_dtypes('number').columns[0]],\n",
    "    description='Input columns',\n",
    "    disabled=False, rows=len(list(pca_df.select_dtypes('number').columns))));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot everything to word document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import os\n",
    "data_of_interest_1 = all_vars\n",
    "data_of_interest_5 = data_of_interest_1.resample('5T').median().interpolate()\n",
    "data_of_interest_1h = data_of_interest_1.resample('1H').median().interpolate()\n",
    "data_of_interest_12 = data_of_interest_1.resample('12H').median().interpolate()\n",
    "document = Document()\n",
    "document.add_heading('Time period overview', level=1)\n",
    "#sections = ['FLUX', 'RESISTANCE', 'PERMEABILITY', 'LAST_TMP']\n",
    "sections = data_of_interest.columns\n",
    "for section in sections:\n",
    "    # cols = [s for s in data_of_interest.columns if section in s]\n",
    "    cols = section\n",
    "    # cols = [s for s in cols if 'LAST_FLUX' not in s]\n",
    "    # for ix, col in enumerate(cols):\n",
    "    #     data_of_interest[col] = data_of_interest[col].loc[~masks[ix]]\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.grid()\n",
    "    plt.title(cols)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.plot(data_of_interest_1[cols], color='k')\n",
    "    plt.plot(data_of_interest_12[cols], color='r')\n",
    "    \n",
    "    plt.savefig('timeseries.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    document.add_picture('timeseries.png', width=Inches(6))\n",
    "    os.remove('timeseries.png')\n",
    "\n",
    "document.save('Timeseries_apr2020.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
