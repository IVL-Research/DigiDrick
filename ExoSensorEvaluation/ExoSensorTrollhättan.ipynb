{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook can be used analyze sensor data and give an estimate on the early warning ability provided that the data is correctly prepared.\n",
    "\n",
    "The following variables are calculated:\n",
    "- True positives\n",
    "- False negatives\n",
    "- False positives\n",
    "- Turbidity peak lengths\n",
    "\n",
    "# Contents\n",
    "- User provided settings\n",
    "- Data import and method declaration\n",
    "- Data exploration and computation of variables\n",
    "- Plots\n",
    "\n",
    "# How to use\n",
    "## Step 1 - Prepare data\n",
    "Prepare your data by creating two excel files, one holding the data and one describing the variables\n",
    "## Step 2 - Run scripts\n",
    "Run each cell in the script using the \"Cell/Run Cells\" button on the top. You can choose to run all cells at once clicking \"Run All\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings\n",
    "User provided input options:\n",
    "\n",
    "- Start date ('yyyy-mm-dd HH:MM:SS')\n",
    "- Stop date ('yyyy-mm-dd HH:MM:SS')\n",
    "- Median sample rate, in minutes. If other than '1', data is median sampled.\n",
    "- File locations for data file (data_file) and observation and variable description file (obsvardef_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2017-01-01 00:00:00'\n",
    "stop_date = '2018-04-01 00:00:00'\n",
    "median_sample_rate = '5'\n",
    "exo_file = 'C:/Users/joel0921/OneDrive - IVL Svenska Miljöinstitutet AB/DigiDrick II/DigiDrickExo/Summary.xlsx'\n",
    "vv_file = 'C:/Users/joel0921/OneDrive - IVL Svenska Miljöinstitutet AB/DigiDrick II/DigiDrickExo/Trollhättan_2017.xlsx'\n",
    "obsvardef_file = 'C:/Users/joel0921/OneDrive - IVL Svenska Miljöinstitutet AB/DigiDrick II/DigiDrickExo/ObsVarDef.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import requests\n",
    "from matplotlib.legend import Legend\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn import linear_model\n",
    "import sklearn.cross_decomposition as skcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_limits(input_data, obsvardef):\n",
    "    output_data = input_data\n",
    "    for col in output_data.columns:\n",
    "        \n",
    "        min_limit = obsvardef['Min limit'].loc[obsvardef['Beskrivning']==col].values[0]\n",
    "        max_limit = obsvardef['Max limit'].loc[obsvardef['Beskrivning']==col].values[0]\n",
    "        if ~np.isnan(min_limit):\n",
    "            output_data[col].loc[output_data[col] < min_limit] = np.nan\n",
    "        if max_limit is not None:\n",
    "            output_data[col].loc[output_data[col] > max_limit] = np.nan\n",
    "        \n",
    "    return output_data\n",
    "\n",
    "\n",
    "def identify_outlier(data, data_column, median_window=24, nbr_of_stds=3, outlier_indicator=True, outlier_bw_offset=0, outlier_fw_offset=0):\n",
    "\n",
    "\n",
    "    if isinstance(data,\n",
    "                  pd.DataFrame):  # If the input is a DataFrame it is treated specially to be able to return a DataFrame\n",
    "        df = True\n",
    "        dataframe = data\n",
    "        # data = data.values  # Convert DataFrame to numpy array\n",
    "    else:\n",
    "        df = False\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "    data['raw_log'] = np.log(data[data_column])  # Natural log , \n",
    "    data['daily_mean_raw_log'] = data['raw_log'].rolling(median_window).median() #.reindex(data.index, method='ffill')\n",
    "    data['daily_stdev_raw_log'] = data['raw_log'].rolling(median_window).std() #.reindex(data.index, method='ffill')\n",
    "    data['log_deviation'] = (data['raw_log'] - data['daily_mean_raw_log']).abs() / data['daily_stdev_raw_log']\n",
    "    data['t+1diff'] = data[data_column].diff(periods=1).abs()\n",
    "    data['t-1diff'] = data[data_column].diff(periods=-1).abs()\n",
    "    data['diff_minmax'] = data[['t-1diff', 't+1diff']].max(axis=1) / data[['t-1diff', 't+1diff']].min(axis=1)\n",
    "    \n",
    "    outlier_col = data_column + '_outliers'\n",
    "    data[outlier_col] = False\n",
    "    data[outlier_col].loc[((data['log_deviation'] > nbr_of_stds) & (data['diff_minmax'] < 2)) | (data['log_deviation'] > nbr_of_stds + 1)] = outlier_indicator\n",
    "    data.drop(['log_deviation', 'diff_minmax', 't-1diff', 't+1diff',\n",
    "              'daily_stdev_raw_log', 'daily_mean_raw_log',\n",
    "              'raw_log'], axis=1, inplace=True)\n",
    "    outliers = data[outlier_col]\n",
    "    for b_ix in range(outlier_bw_offset):\n",
    "        outliers += outliers.shift(-1*b_ix)\n",
    "    for f_ix in range(outlier_fw_offset):\n",
    "        outliers += outliers.shift(f_ix)\n",
    "\n",
    "    data[outlier_col] = outliers > 0\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import exo-sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = pd.ExcelFile(exo_file)\n",
    "sheet_names = xl.sheet_names\n",
    "sensor_data = pd.DataFrame()\n",
    "count = 0 \n",
    "for sheet in sheet_names:\n",
    "    print('Importing: ' + sheet)\n",
    "    df = pd.read_excel(exo_file, sheet_name=sheet, index_col=0, header=0)  # Import data file to pandas Dataframe\n",
    "    dt = [datetime.datetime.utcfromtimestamp(int(ts)).strftime('%Y-%m-%d %H:%M:%S') for ts in df.index]\n",
    "    df.index = pd.to_datetime(dt)\n",
    "    df.columns = df.columns + '_' + sheet\n",
    "    if count == 0:\n",
    "        sensor_data = df\n",
    "    else:\n",
    "        sensor_data = pd.merge(sensor_data, df, left_index=True, right_index=True, how='inner')\n",
    "    count += 1\n",
    "    \n",
    "sensor_data.drop_duplicates()    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import site data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = pd.ExcelFile(vv_file)\n",
    "sheet_names = xl.sheet_names\n",
    "vv_data = pd.DataFrame()\n",
    "for sheet in sheet_names:\n",
    "    print('Importing: ' + sheet)\n",
    "    df = pd.read_excel(vv_file, sheet_name=sheet)  # Import data file to pandas Dataframe\n",
    "    df.index = df['Time']\n",
    "    df.drop(['Time'], axis=1)\n",
    "    vv_data = pd.concat([vv_data, df])\n",
    "    \n",
    "vv_data.drop_duplicates()    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-process data (median filter and exclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsvardef_data = pd.read_excel(obsvardef_file)\n",
    "\n",
    "vv = vv_data.resample(median_sample_rate + 'T').median()\n",
    "exo = sensor_data.resample(median_sample_rate + 'T').median()\n",
    "joined_data = pd.merge(vv, exo, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "data_limits_applied = exclude_limits(joined_data, obsvardef_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, Layout\n",
    "import cufflinks as cf\n",
    "import plotly.offline as py\n",
    "cf.go_offline()\n",
    "\n",
    "start_date = widgets.DatePicker(description='Start', disabled=False, value=pd.to_datetime('2017-01-01'))\n",
    "display(start_date)\n",
    "stop_date = widgets.DatePicker(description='Stop', disabled=False, value=pd.to_datetime('2018-01-01'))\n",
    "display(stop_date)\n",
    "\n",
    "median_slider = widgets.IntSlider(min=5, max=60, step=1, value=10, description='Median filter [min]')\n",
    "display(median_slider)\n",
    "\n",
    "y_list = widgets.SelectMultiple(\n",
    "    options=list(data_limits_applied.select_dtypes('number').columns),\n",
    "    value=[data_limits_applied.select_dtypes('number').columns[0]],\n",
    "    description='Input columns',\n",
    "    disabled=False)\n",
    "\n",
    "@interact\n",
    "def update_plot(y=y_list):\n",
    "    \n",
    "    df = data_limits_applied.resample(str(median_slider.value) + 'T').median()\n",
    "    df = df.loc[start_date.value:stop_date.value]\n",
    "    plot_df = df[list(y)]\n",
    "    display(plot_df.iplot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify outliers\n",
    "Select which signal to use for outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "window_slider = widgets.IntSlider(min=10, max=60, step=1, value=24, description='Median filter window [time steps]')\n",
    "display(window_slider)\n",
    "\n",
    "@interact\n",
    "def update_plot(outlier_signal=list(data_limits_applied.columns)):\n",
    "    \n",
    "    data = data_limits_applied.copy()\n",
    "    data = identify_outlier(data, outlier_signal, window_slider.value, nbr_of_stds=3)\n",
    "    \n",
    "    plot_df = pd.DataFrame()\n",
    "    plot_df[outlier_signal] = data[outlier_signal]\n",
    "    outliers = data[outlier_signal + '_outliers']\n",
    "    \n",
    "    plot_df['Outliers Removed'] = data[outlier_signal].loc[~outliers]\n",
    "    data_limits_applied[outlier_signal + '_OutliersRemoved'] = data[outlier_signal]\n",
    "    data_limits_applied[outlier_signal + '_OutliersRemoved'].loc[outliers] = np.nan\n",
    "    \n",
    "    display(plot_df.iplot())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find cross-correlations\n",
    "Cross correlations can be used to determine the time-lag between signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(datax, datay, lag=0):\n",
    "    \"\"\" Lag-N cross correlation. \n",
    "    Parameters\n",
    "    ----------\n",
    "    lag : int, default 0\n",
    "    datax, datay : pandas.Series objects of equal length\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    crosscorr : float\n",
    "    \"\"\"\n",
    "    return datax.corr(datay.shift(lag))\n",
    "\n",
    "\n",
    "start_date = data_limits_applied.index[0]\n",
    "end_date = data_limits_applied.index[-1]\n",
    "duration = end_date - start_date\n",
    "duration_months = round(duration.days/30)\n",
    "\n",
    "datelist = pd.date_range(start_date, periods=duration_months+1, freq='1M').tolist()\n",
    "points = 72\n",
    "t_vec = []\n",
    "\n",
    "@interact\n",
    "def update_plot(y_1=list(data_limits_applied.columns), y_2=list(data_limits_applied.columns)):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for day_ix in range(len(datelist)-1):\n",
    "        mask = (data_limits_applied.index > datelist[day_ix]) & (data_limits_applied.index <= datelist[day_ix + 1])\n",
    "        datay = data_limits_applied[y_1].loc[mask]\n",
    "        datax = data_limits_applied[y_2].loc[mask]\n",
    "        xcov = [crosscorr(datax, datay, lag=i) for i in range(-1*points, points)]\n",
    "        plt.plot(range(-1*points, points), xcov, label='Period: ' + str(datelist[day_ix]) + ' - ' + str(datelist[day_ix + 1]) )\n",
    "        t_vec.append(xcov.index(max(xcov)))\n",
    "    \n",
    "    plt.xlabel('Time shift [ each step is ' + median_sample_rate + ' minutes]')\n",
    "    plt.ylabel('CrossCorrelation')\n",
    "    plt.grid(True, which='major')\n",
    "    plt.grid(True, which='minor', linestyle='--')\n",
    "    plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate early warning capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "\n",
    "def filter_out_anomalies(random_data):\n",
    "    \n",
    "    \n",
    "    # Set upper and lower limit to 3 standard deviation\n",
    "    random_data_std = np.std(random_data)\n",
    "    random_data_mean = np.mean(random_data)\n",
    "    anomaly_cut_off = random_data_std * 3\n",
    "    \n",
    "    lower_limit  = random_data_mean - anomaly_cut_off \n",
    "    upper_limit = random_data_mean + anomaly_cut_off\n",
    "    \n",
    "    # Never allow data points below 0\n",
    "    if lower_limit < 0:\n",
    "        lower_limit = 0\n",
    "    \n",
    "    temp = random_data[random_data > lower_limit]\n",
    "    out = temp[temp < upper_limit]\n",
    "    #print(\"Removed values over \" + str(upper_limit) + \" and values under \" + str(lower_limit))\n",
    "    return out\n",
    "\n",
    "\n",
    "def is_value_in_interval(val, range1, range2):\n",
    "    return range1 <= val <= range2\n",
    "\n",
    "\n",
    "def measure_overlap(data1,data2):\n",
    "    latest_start = max(data1[0], data2[0])\n",
    "    earliest_end = min(data1[1], data2[1])\n",
    "    delta = earliest_end-latest_start + 1\n",
    "    overlap = max(0,delta)\n",
    "    return overlap\n",
    "\n",
    "\n",
    "def peak_finder(data1, data2, resample_frequency, smoothing_factor = 300, peak_limit = 7, peak_width = 120, plot=False):\n",
    "    print(\"%%%%%%%%%%%%%%% PARAMETERS %%%%%%%%%%%%%%%\")\n",
    "    print(\"Finding peaks with the following settings:\")\n",
    "    print(\"Smoothing windows size: \" + str(smoothing_factor))\n",
    "    print(\"Resample frequency: \" + str(resample_frequency))\n",
    "    print(\"Minimum peak limit: \" + str(peak_limit))\n",
    "    print(\"Minimum peak width: \" + str(peak_width))\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "      \n",
    "    \n",
    "    # Array of data2 values over peak limit\n",
    "    #data2_over_pl_all_points = (data2 > peak_limit).values\n",
    "    \n",
    "    # Filter out anomalies\n",
    "    data1_col = data1.name\n",
    "    #data1 = identify_outlier(data1, data1_col, 100, nbr_of_stds=3)\n",
    "    #data1 = data1[data1_col].loc[~data1[data1_col + '_outliers']]\n",
    "    \n",
    "    data2_col = data2.name\n",
    "    #data2 = identify_outlier(data2, data2_col, 100, nbr_of_stds=3)\n",
    "    #data2 = data2[data2_col].loc[~data2[data2_col + '_outliers']]\n",
    "    \n",
    "    # Store times. TODO: Plot against these values!\n",
    "    data1_time = data1.index\n",
    "    data2_time = data2.index\n",
    "    data1_start_stop = [data1_time[0], data1_time[-1]]\n",
    "    data2_start_stop = [data2_time[0], data2_time[-1]]\n",
    "    \n",
    "    # Smooth (moving average with smoothing_factor = window size)\n",
    "    data1 = smooth(data1, smoothing_factor)\n",
    "    data2 = smooth(data2, smoothing_factor)\n",
    "    \n",
    "    # Find peaks\n",
    "    data1_peaks, data1_peak_properties = find_peaks(data1, width = peak_width, height = peak_limit)\n",
    "    data2_peaks, data2_peak_properties = find_peaks(data2, width = peak_width, height = peak_limit)\n",
    "    \n",
    "    # convert time frequency to integer\n",
    "    time_resolution = resample_frequency # minutes\n",
    "    \n",
    "    # Intervals of each peak in data2\n",
    "    data2_peak_starts = data2_peaks - data2_peak_properties['widths']/2\n",
    "    data2_peak_ends = data2_peaks + data2_peak_properties['widths']/2\n",
    "    p2_intervals = list(zip(data2_peak_starts, data2_peak_ends))\n",
    "\n",
    "    # Intervals of each peak in data1, with one hour cutoff in the beginning of data1 peaks\n",
    "    p1_intervals = list(zip(data1_peaks - data1_peak_properties['widths']/2 + 60/time_resolution, \n",
    "                               data1_peaks + data1_peak_properties['widths']/2))# - 60/time_resolution))\n",
    "\n",
    "    p2_p1_matches = list()\n",
    "    for interval in p2_intervals: # Each data2 peak interval\n",
    "        temp_p2_start = int(interval[0])\n",
    "        temp_p2_end = int(interval[1])\n",
    "\n",
    "        # Check if start OR end point of p2 interval is within p1 interval\n",
    "        true_false_tuple_list = [(is_value_in_interval(temp_p2_start, p1_interval[0], p1_interval[1]), \n",
    "          is_value_in_interval(temp_p2_end, p1_interval[0], p1_interval[1])) for p1_interval in p1_intervals]\n",
    "\n",
    "        p2_peaks_in_p1 = [a or b for (a,b) in true_false_tuple_list]\n",
    "        p2_p1_matches.append(p2_peaks_in_p1)\n",
    "    \n",
    "    # Check which peaks were found and store the results in peak properties\n",
    "    all_matches = np.array([], dtype=bool)\n",
    "    for match in p2_p1_matches:\n",
    "        all_matches = np.append(all_matches, bool(np.sum(match)))\n",
    "    data2_peak_properties['predicted'] = all_matches    \n",
    "    \n",
    "    # Array of data2 values over peak limit\n",
    "    dd_2 = np.zeros(len(data2),dtype=bool)\n",
    "    for interval in p2_intervals:\n",
    "        start = int(interval[0])\n",
    "        stop = int(interval[1])\n",
    "        dd_2[start:stop] = True\n",
    "    \n",
    "    data2_over_pl_all_points = dd_2\n",
    "    \n",
    "    # Calculate ratio of predicted peaks based on data1 that are found\n",
    "    dd = np.zeros(len(data2_over_pl_all_points),dtype=bool)\n",
    "    for interval in p1_intervals:\n",
    "        start = int(interval[0])\n",
    "        stop = int(interval[1])\n",
    "        dd[start:stop] = True\n",
    "    \n",
    "    true_positives = np.zeros(len(data2_over_pl_all_points),dtype=bool)\n",
    "    false_positives = np.zeros(len(data2_over_pl_all_points),dtype=bool)\n",
    "    false_negatives = np.zeros(len(data2_over_pl_all_points),dtype=bool)    \n",
    "    for idx in range(len(dd)):\n",
    "        true_positives[idx] = all([dd[idx],data2_over_pl_all_points[idx]])\n",
    "        false_positives[idx] =  dd[idx] and not data2_over_pl_all_points[idx]\n",
    "        false_negatives[idx] = data2_over_pl_all_points[idx] and not dd[idx]\n",
    "    pred_ratio = np.sum(true_positives)/np.sum(data2_over_pl_all_points)\n",
    "    false_positive_rate = np.sum(false_positives)/np.sum(data2_over_pl_all_points)\n",
    "    false_negative_rate = np.sum(false_negatives)/np.sum(data2_over_pl_all_points)\n",
    "    \n",
    "    # Measure overlap of intervals\n",
    "    total_overlap = 0\n",
    "    for p1_interval in p1_intervals:\n",
    "        for p2_interval in p2_intervals:\n",
    "            total_overlap += measure_overlap(p1_interval,p2_interval)\n",
    "    overlap_p2_fraction = total_overlap/np.sum(data2_peak_properties['widths'])\n",
    "    overlap_p1_fraction = total_overlap/np.sum(data1_peak_properties['widths'])\n",
    "\n",
    "    # Plot\n",
    "    if plot:\n",
    "        # Plot the data, as it is filtered above.\n",
    "        start = pd.Timestamp(data1_time[0])\n",
    "        end = pd.Timestamp(data1_time[-1])\n",
    "        t = np.linspace(start.value, end.value, len(data1))\n",
    "        t = pd.to_datetime(t)\n",
    "        start_d2 = pd.Timestamp(data2_time[0])\n",
    "        end_d2 = pd.Timestamp(data2_time[-1])\n",
    "        t2 = np.linspace(start_d2.value, end_d2.value, len(data2))\n",
    "        t2 = pd.to_datetime(t2)        \n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(t, data1, label = data1_col)\n",
    "        plt.plot(t2, data2, label = data2_col)\n",
    "        plt.title(\"Data 1 & 2 with marker sizes being relative to width of the peaks\",fontsize=20)\n",
    "        \n",
    "        # Plot markers at peaks with marker sizes relative to width of peaks.\n",
    "        data1_marker_sizes = data1_peak_properties['widths']/max(data1_peak_properties['widths'])*50\n",
    "        data2_marker_sizes = data2_peak_properties['widths']/max(data2_peak_properties['widths'])*50\n",
    "        for idx, point in enumerate(data1_peaks):\n",
    "            plt.plot(t[point], data1[point], '.',markersize=data1_marker_sizes[idx],color=\"C0\")\n",
    "        for idx, point in enumerate(data2_peaks):\n",
    "            plt.plot(t2[point], data2[point], '.',markersize=data2_marker_sizes[idx],color=\"C1\")\n",
    "        plt.legend(fontsize=15,loc=1)\n",
    "        \n",
    "        # Plot only markers in time order with size being relative to width of peaks\n",
    "        plt.figure(figsize=(20,5))\n",
    "        for idx, point in enumerate(data1_peaks):\n",
    "            plt.plot(t[point], 1, '.',markersize=data1_marker_sizes[idx],color=\"C0\")\n",
    "        for idx, point in enumerate(data2_peaks):\n",
    "            plt.plot(t2[point], 0, '.',markersize=data2_marker_sizes[idx],color=\"C1\")\n",
    "        \n",
    "        # Plot true positives green and true negatives red\n",
    "        plot_linspace = np.linspace(start.value,end.value,len(data2_over_pl_all_points))    \n",
    "        plot_linspace = pd.to_datetime(plot_linspace)\n",
    "        plt.plot(plot_linspace[data2_over_pl_all_points], np.repeat(-1,len(plot_linspace))[data2_over_pl_all_points],'r',marker='.',linestyle = 'None',markersize=13)\n",
    "        plt.plot(plot_linspace[true_positives], np.repeat(-1,len(plot_linspace))[true_positives],'g',marker='.',linestyle = 'None',markersize=13)\n",
    "        plt.title(\"Peaks ranged in time order with marker size being relative to the width of the peaks\",fontsize=20)\n",
    "        custom_lines = [Line2D([0], [0], color='C0', marker=\".\",linestyle = 'None'),\n",
    "                        Line2D([0], [0], color='C1', marker='.',linestyle = 'None'),\n",
    "                       Line2D([0], [0], color='g', marker='.',linestyle = 'None'),\n",
    "                       Line2D([0], [0], color='r', marker='.',linestyle = 'None')]\n",
    "        plt.legend(custom_lines, [data1_col, data2_col, 'Values over peak limit in ' + data2_col + ' and predicted',\n",
    "                                  'Values over peak limit in ' + data2_col + ' but not predicted'],fontsize=15,loc=1)\n",
    "        plt.ylim([-2,4])\n",
    "        plt.yticks([])\n",
    "    \n",
    "    ratio_number_of_peaks = np.sum(data2_peak_properties['predicted'])/len(data2_peaks)\n",
    "    \n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RESULTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "    print(\"Ratio of values over \" + str(peak_limit) + \" in \" + data2_col + \" found as peaks in \" + data1_col + \" (True positives): \" + str(round(pred_ratio,2)))\n",
    "    print(\"Ratio of values over \" + str(peak_limit) + \" in \" + data2_col + \" not found as peaks in \" + data1_col + \" (False negatives): \" + str(round(false_negative_rate,2)))\n",
    "    print(\"Ratio of values under \" + str(peak_limit) + \" in \" + data2_col + \" found as peaks in \" + data1_col + \" (False positives): \" + str(round(false_positive_rate,2)))\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "    \n",
    "    return [data1_peaks,data1_peak_properties, data2_peaks, data2_peak_properties]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Early warning evaluation\n",
    "- Signal 1: the earlier signal to be evaluated\n",
    "- Signal 2: the later signal used as \"truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "start_date = widgets.DatePicker(description='Start', disabled=False, value=pd.to_datetime('2017-01-01'))\n",
    "display(start_date)\n",
    "stop_date = widgets.DatePicker(description='Stop', disabled=False, value=pd.to_datetime('2018-01-01'))\n",
    "display(stop_date)\n",
    "\n",
    "median_slider = widgets.IntSlider(min=5, max=60, step=1, value=10, description='Median filter [min]')\n",
    "display(median_slider)\n",
    "\n",
    "peak_width_slider = widgets.IntSlider(min=10, max=200, step=10, value=50, description='Peak width [min]')\n",
    "display(peak_width_slider)\n",
    "\n",
    "peak_limit_slider = widgets.IntSlider(min=5, max=50, step=1, value=7, description='Peak limit')\n",
    "display(peak_limit_slider)\n",
    "\n",
    "\n",
    "@interact\n",
    "def update_plot(Signal_1=list(data_limits_applied.columns), Signal_2=list(data_limits_applied.columns)):\n",
    "    \n",
    "    \n",
    "    df = data_limits_applied.resample(str(median_slider.value) + 'T').median()\n",
    "    df = df.loc[start_date.value:stop_date.value]\n",
    "    peak_finder(df[Signal_1], df[Signal_2], median_slider.value, plot = True, smoothing_factor = 50, peak_limit = peak_limit_slider.value, peak_width = peak_width_slider.value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "joined_data_inner['Turbidity_FNU'].loc[joined_data_inner['Turbidity_FNU'] < 0 ] = np.nan\n",
    "joined_data_inner['Turbidity_FNU'].loc[joined_data_inner['Turbidity_FNU'] > 100] = np.nan\n",
    "\n",
    "rv_limit = 7\n",
    "exo_limit = 7\n",
    "min_length = 4\n",
    "rv_mask = joined_data_inner['Turbiditetet råvatten ISCAN'] > rv_limit\n",
    "exo_mask = joined_data_inner['Turbidity_FNU'] > exo_limit\n",
    "\n",
    "### ISCAN ###\n",
    "rv_mask = np.array(rv_mask) * 1\n",
    "rv_mask[np.isnan(rv_mask)] = 1\n",
    "\n",
    "for ix in range(min_length, len(rv_mask)):\n",
    "    if (rv_mask[ix]==1) & (sum(rv_mask[ix-min_length:ix])>=1):\n",
    "        rv_mask[ix-min_length:ix] = 1\n",
    "\n",
    "cumsum = np.cumsum(rv_mask)\n",
    "cumsum_diff = np.diff(cumsum)\n",
    "\n",
    "start_ix_rv = []\n",
    "start_time_rv = []\n",
    "stop_ix_rv = []\n",
    "stop_time_rv = []\n",
    "\n",
    "for ix in range(1, len(cumsum_diff)):\n",
    "    if (cumsum_diff[ix-1]==0) & (cumsum_diff[ix]==1):  \n",
    "        start_ix_rv.append(ix)\n",
    "        start_time_rv.append(joined_data_inner.index[ix])\n",
    "    elif (cumsum_diff[ix-1]==1) & (cumsum_diff[ix]==0):\n",
    "        stop_ix_rv.append(ix+1)\n",
    "        stop_time_rv.append(joined_data_inner.index[ix+1])\n",
    "\n",
    "if len(start_time_rv) > len(stop_time_rv):\n",
    "    start_time_rv = start_time_rv[:-1]\n",
    "    start_ix_rv = start_ix_rv[:-1]\n",
    "\n",
    "cum_turb_rv = []\n",
    "peak_length_rv = []\n",
    "for ix in range(len(stop_ix_rv)):\n",
    "    peak_length_rv.append(stop_ix_rv[ix]-start_ix_rv[ix])\n",
    "    cum_turb_rv.append(sum(joined_data_inner['Turbiditetet råvatten ISCAN'].loc[start_time_rv[ix]:stop_time_rv[ix]]))\n",
    "\n",
    "        \n",
    "plt.figure()\n",
    "plt.plot(joined_data_inner['Turbiditetet råvatten ISCAN'])\n",
    "plt.plot(joined_data_inner['Turbiditetet råvatten ISCAN'].loc[start_time_rv], 'r*')\n",
    "plt.plot(joined_data_inner['Turbiditetet råvatten ISCAN'].loc[stop_time_rv], 'k*')\n",
    "plt.show()\n",
    "\n",
    "### EXO ###\n",
    "exo_mask = np.array(exo_mask) * 1\n",
    "exo_mask[np.isnan(exo_mask)] = 1\n",
    "\n",
    "for ix in range(min_length, len(exo_mask)):\n",
    "    if (exo_mask[ix]==1) & (sum(exo_mask[ix-min_length:ix])>=1):\n",
    "        exo_mask[ix-min_length:ix] = 1\n",
    "\n",
    "cumsum = np.cumsum(exo_mask)\n",
    "cumsum_diff = np.diff(cumsum)\n",
    "\n",
    "start_ix_exo = []\n",
    "start_time_exo = []\n",
    "stop_ix_exo = []\n",
    "stop_time_exo = []\n",
    "\n",
    "for ix in range(1, len(cumsum_diff)):\n",
    "    if (cumsum_diff[ix-1]==0) & (cumsum_diff[ix]==1):  \n",
    "        start_ix_exo.append(ix)\n",
    "        start_time_exo.append(joined_data_inner.index[ix])\n",
    "    elif (cumsum_diff[ix-1]==1) & (cumsum_diff[ix]==0):\n",
    "        stop_ix_exo.append(ix+1)\n",
    "        stop_time_exo.append(joined_data_inner.index[ix+1])\n",
    "\n",
    "if len(start_time_exo) > len(stop_time_exo):\n",
    "    start_time_exo = start_time_exo[:-1]\n",
    "    start_ix_exo = start_ix_exo[:-1]\n",
    "        \n",
    "cum_turb_exo = []\n",
    "peak_length_exo = []\n",
    "for ix in range(len(stop_ix_exo)):\n",
    "    peak_length_exo.append(stop_ix_exo[ix]-start_ix_exo[ix])\n",
    "    cum_turb_exo.append(sum(joined_data_inner['Turbidity_FNU'].loc[start_time_exo[ix]:stop_time_exo[ix]]))\n",
    "        \n",
    "plt.figure()\n",
    "plt.plot(joined_data_inner['Turbidity_FNU'])\n",
    "plt.plot(joined_data_inner['Turbidity_FNU'].loc[start_time_exo], 'r*')\n",
    "plt.plot(joined_data_inner['Turbidity_FNU'].loc[stop_time_exo], 'k*')\n",
    "plt.ylim([0, 25])\n",
    "plt.legend(['Exo data', 'Peak start', 'Peak stop'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Cumulative turbidity')\n",
    "plt.plot(start_time_rv, cum_turb_rv, 'r*')\n",
    "plt.plot(start_time_exo, cum_turb_exo, 'k*')\n",
    "plt.legend(['ISCAN', 'EXO'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Peak length')\n",
    "plt.plot(start_time_rv, peak_length_rv,'r*')\n",
    "plt.plot(start_time_exo, peak_length_exo,'k*')\n",
    "plt.legend(['ISCAN', 'EXO'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
